# ğŸš€ Enhanced Bidirectional MRC-based ABSA System
This repository provides an enhanced Bidirectional MRC-based model designed for the DimABSA 2025 Task 2 & Task 3 benchmark, supporting Aspect-Opinion extraction and Aspect-Opinion-Category-Valence-Arousal Quadruplet extraction across multiple domains and languages.

# âœ¨ Key Features
| Enhancement                          | Description                                  |
| ------------------------------------ | -------------------------------------------- |
| **FGM / PGD Adversarial Training**   | æå‡æ¨¡å‹å°å°æŠ—æ¨£æœ¬çš„é­¯æ£’æ€§                      |
| **EMA (Exponential Moving Average)** | æ¸›å°‘è¨“ç·´ä¸ç©©å®šèˆ‡éæ“¬åˆ                                  |
| **R-Drop Regularization**            | å¢å¼·åˆ†é¡é‚è¼¯ä¸€è‡´æ€§                                    |
| **Category Loss Enhanced**           | Focal Loss + Label Smoothing + Class Weights |
| **Cosine Warmup Scheduler**          | æ›´å¹³æ»‘çš„å­¸ç¿’ç‡ç­–ç•¥                                    |
| **Multi-GPU Support**                | è‡ªå‹•åµæ¸¬ GPU æ•¸é‡ä¸¦å¹³è¡Œè¨“ç·´                             |
| **Post-processing Optimization**     | Category refinementï¼ˆèªç¾©è¼”åŠ©ï¼‰æå‡ Task3 æº–ç¢ºåº¦        |

# ğŸ“¦ Installation
git clone https://github.com/YourRepo/ABSA-Enhanced.git
cd ABSA-Enhanced

# ğŸ“‚ Directory Structure
.
â”œâ”€ data/                # Dataset inputs
â”œâ”€ tasks/        # Inference output results
â”œâ”€ model/        # Saved checkpoints
â”œâ”€ log/                 # Training logs
â”œâ”€ DataProcess.py
â”œâ”€ DimABSAModel.py
â”œâ”€ Utils.py
â”œâ”€ NLP.py
â”œâ”€ adversarial_training.py
â”œâ”€ category_loss_enhanced.py
â”œâ”€ data_augmentation.py
â”œâ”€ download_model.py
â”œâ”€ ema.py
â”œâ”€ ensemble.py
â”œâ”€ focal_loss.py
â”œâ”€ postprocess_optimizer.py
â”œâ”€ pred_zho_restaurant_0.5825.jsonl
â”œâ”€ rdrop.py
â””â”€ README.md

# â–¶ï¸ How to Run
## â­ Train

python train.py \
    --task 3 \
    --domain res \
    --language zho \
    --mode train \
    --epoch_num 6 \
    --epoch_num 40 \
    --batch_size 2 \
    --gradient_accumulation_steps 8 \
    --learning_rate 5e-5 \
    --tuning_bert_rate 5e-5
    --use_fgm True \
    --adv_epsilon 1.5 \
    --use_ema True \
    --ema_decay 0.9998 \
    --use_focal_loss True \
    --focal_gamma 3.0 \
    --label_smoothing 0.2 \
    --beta 2.0 \
    --scheduler_type cosine \
    --inference_beta 0.9 \

## ğŸ§ª Evaluate

python run_task2_3_trainer_enhanced.py \
    --mode evaluate
    
## ğŸ“˜ Inference

python run_task2_3_trainer_enhanced.py \
    --mode inference
    
è¼¸å‡ºå°‡è‡ªå‹•å„²å­˜æ–¼ï¼š
```bash
tasks_reduce/subtask_2/*.jsonl
tasks_reduce/subtask_3/*.jsonl
```

# ğŸ“Š Performance

| Epoch | Learning rate | adv_epsilon | label_smoothing | ema_decay | focal_gamma | beta | inference_beta  | drop_alpha | æœªè¼¸å‡ºæ•¸é‡ | cF1 | 
| ------------- | ------ | ------ | ----------------- | ------------- | ------ | ------ | ----------------- | ----------------- |----------------- |
| 3 | 1e-3 | x | x | x | x | 1 | 0.9 | 4.0 | 16 | 0.5757 | 
| 20 | 2e-5 | 1.0 | 0.2 | 0.999 | 2.5 | 1.5 | 0.82  | 4.0 | 5 | 0.5393 | 
| 30 | 2e-5 | 1.0 | 0.2 | 0.999 | 2.5 | 1.5 | 0.82  | 4.0 | 5 | 0.5561 |  
| 50 | 2e-5 | 1.0 | 0.2 | 0.999 | 2.5 | 1.5 | 0.82  | 4.0 | 5 | 0.5595 | 
| 40 | 2.5e-5 | 1.8 | 0.1 | 0.999 | 2 | 2 | 0.78  | 4.0 | 5 | 0.5471 | 
| 40 | 2e-5 | 1.4 | 0.15 | 0.9995 | 2.5 | 2 | 0.83  | 4.0 | 5 | 0.5587 | 
| 40 | 2e-5 | 1.5 | 0.2 | 0.9998 | 3 | 2 | 0.88  | 4.0 | 6 | 0.5640 | 
| 40 | 5e-5 | 1.5 | 0.2 | 0.9998 | 3 | 2 | 0.9  | 1.0 | 9 | 0.5825 | 





